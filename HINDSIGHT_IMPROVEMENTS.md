# Chain of Hindsight: æ·±æ˜ã‚Šæ”¹å–„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

## æ¦‚è¦

å‰å›å®Ÿè£…ã—ãŸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼ˆPath Bï¼‰ã‚’æŒ¯ã‚Šè¿”ã‚Šã€**å¾ŒçŸ¥æµï¼ˆHindsightï¼‰** ã«ã‚ˆã£ã¦éš ã‚Œã¦ã„ãŸå•é¡Œã‚’ç™ºè¦‹ã—ã€å®Œå…¨ã«ä¿®æ­£ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã™ã€‚

---

## ç¬¬1æ®µéš: åˆå›æ¤œè¨ï¼ˆInitial Attemptï¼‰

### å‰å›å®Ÿè£…ã®å†…å®¹
- `@lru_cache` ã«ã‚ˆã‚‹ token2id ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
- ThreadPoolExecutor ã«ã‚ˆã‚‹ä¸¦åˆ—å‡¦ç†
- ãƒãƒƒãƒå‡¦ç†å‘ã‘ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
- æ”¹å–„ç‡: **-27%** ï¼ˆç†è«–å€¤ï¼‰

### æœŸå¾…ã•ã‚ŒãŸåŠ¹æœ
```
1000 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†:
- å®Ÿè¡Œæ™‚é–“: 8.5s â†’ 6.2s (27% å‰Šæ¸›)
- ãƒ¡ãƒ¢ãƒª: 850MB â†’ 780MB (8% å‰Šæ¸›)
```

---

## ç¬¬2æ®µéš: å¾ŒçŸ¥æµåˆ†æï¼ˆHindsight Analysisï¼‰

### ğŸ”´ ç™ºè¦‹1: ä¸¦åˆ—åŒ–ã®å®ŸåŠ¹æ€§ãŒæœªæ¤œè¨¼

**å•é¡Œ**:
```python
def _encode_single_doc(encoder, doc, token2id, doc_id):
    with torch.inference_mode():
        embedding = encoder.encode([doc])  # â† 1 å€‹ãšã¤å‡¦ç†
        sparse_vector = encoder.to_sparse(embedding)[0]
```

- PyTorch ãƒãƒƒãƒåŒ–ã«ã‚ˆã‚‹åˆ©ç›Šã‚’å¤±ã†
- å„ã‚¹ãƒ¬ãƒƒãƒ‰ãŒç‹¬ç«‹ã—ã¦ `encode()` ã‚’å‘¼ã¶ãŸã‚ã€GPU/CPU ã®åŠ¹ç‡ãŒä½ä¸‹
- **ã‚¹ãƒ¬ãƒƒãƒ‰ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰** ãŒè¨ˆç®—æ™‚é–“ã‚’ä¸Šå›ã‚‹å¯èƒ½æ€§

**ä¿®æ­£æ–¹æ³•**:
â†’ **ãƒãƒ£ãƒ³ã‚¯å˜ä½ã®ä¸¦åˆ—å‡¦ç†** ã«å¤‰æ›´
```python
# è¤‡æ•°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã¾ã¨ã‚ã¦ 1 ã¤ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§å‡¦ç†
chunks = [docs[i:i+chunk_size] for i in range(0, len(docs), chunk_size)]
for chunk in executor.map(_encode_batch_documents, chunks):
    ...
```

---

### ğŸ”´ ç™ºè¦‹2: ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã®æ¤œè¨¼ãªã—

**å•é¡Œ**:
```python
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [
        executor.submit(_encode_single_doc, encoder, doc, token2id, i)
        for i, doc in enumerate(docs)
    ]
```

- `SpladeEncoder` ãŒã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã‹æœªæ¤œè¨¼
- è¤‡æ•°ã‚¹ãƒ¬ãƒƒãƒ‰ã§åŒã˜ encoder ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å…±æœ‰
- **Race condition** ã®ãƒªã‚¹ã‚¯

**ä¿®æ­£æ–¹æ³•**:
â†’ **ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸** ã§ encoder ã‚’åˆ†é›¢
```python
_encoder_local = threading.local()

def get_or_create_thread_local_encoder(encoder: SpladeEncoder):
    if not hasattr(_encoder_local, 'encoder'):
        _encoder_local.encoder = encoder
    return _encoder_local.encoder
```

---

### ğŸ”´ ç™ºè¦‹3: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ID ã®é †åºãƒã‚°

**å•é¡Œ**:
```python
futures = [executor.submit(..., i) for i, doc in enumerate(docs)]
points = [f.result() for f in futures]  # â† Future å®Œäº†é †ã«å–å¾—
```

Future ã®å®Œäº†é †ã¯ **å®Ÿè¡Œæ™‚é–“ã«ä¾å­˜** ã™ã‚‹ãŸã‚ã€çµæœã®é †åºãŒç‹‚ã†å¯èƒ½æ€§

**å…·ä½“ä¾‹**:
```
æå‡ºé †:  doc[0], doc[1], doc[2], doc[3], doc[4]
å®Œäº†é †:  doc[2] (é€Ÿ), doc[0] (é…), doc[1], doc[4], doc[3]

çµæœ: points = [doc[2]_result, doc[0]_result, ...]  âŒ é †åºãŒç‹‚ã†ï¼
```

**ä¿®æ­£æ–¹æ³•**:
â†’ **ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«é †åºã‚’ä¿æŒ**
```python
from concurrent.futures import as_completed

future_to_chunk_idx = {...}
points_dict = {}

for future in as_completed(future_to_chunk_idx):
    chunk_idx = future_to_chunk_idx[future]
    points_dict[chunk_idx] = future.result()

# å…ƒã®é †åºã§å†æ§‹ç¯‰
points = [points_dict[i] for i in sorted(points_dict.keys())]
```

---

### ğŸŸ¡ ç™ºè¦‹4: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¹ã‚³ãƒ¼ãƒ—ãŒä¸é©åˆ‡

**å•é¡Œ**:
```python
@lru_cache(maxsize=1)
def get_token2id(tokenizer) -> dict[str, int]:
    return tokenizer.get_vocab()
```

- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ãŒ `tokenizer` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã® identity
- ç•°ãªã‚‹ encoder ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹
- ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«ç’°å¢ƒã§éåŠ¹ç‡

**ä¿®æ­£æ–¹æ³•**:
â†’ **ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ã‚­ãƒ¼ã¨ã™ã‚‹æ˜ç¤ºçš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥**
```python
_token2id_cache: Dict[str, dict] = {}
_cache_lock = threading.Lock()

def get_cached_token2id(model_path: str, tokenizer):
    with _cache_lock:
        if model_path not in _token2id_cache:
            _token2id_cache[model_path] = tokenizer.get_vocab()
        return _token2id_cache[model_path]
```

---

### ğŸŸ¡ ç™ºè¦‹5: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ID è¡çªãƒªã‚¹ã‚¯

**å•é¡Œ**:
```python
# é€šå¸¸å‡¦ç†
points_1 = encode_documents2points(encoder, docs_1)  # id: 0-4

# ãƒãƒƒãƒå‡¦ç†
for batch in encode_documents2points_batched(encoder, docs_2):
    # id ãŒ 0 ã‹ã‚‰å§‹ã¾ã‚‹ â†’ docs_1 ã¨ ID é‡è¤‡ï¼
```

Qdrant ã§ ID ãŒä¸Šæ›¸ãã•ã‚Œã‚‹

**ä¿®æ­£æ–¹æ³•**:
â†’ **ID ã‚ªãƒ•ã‚»ãƒƒãƒˆç®¡ç†**
```python
def encode_documents2points_batched(
    encoder, docs, batch_size=1000, id_offset=0
):
    for batch_start in range(0, len(docs), batch_size):
        batch_offset = id_offset + batch_start
        # batch_offset ã‚’ä½¿ç”¨ã—ã¦ä¸€æ„ãª ID ã‚’ç”Ÿæˆ
```

---

### ğŸŸ¡ ç™ºè¦‹6: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ¬ å¦‚

**å•é¡Œ**:
- ã‚¹ãƒ¬ãƒƒãƒ‰å†…ä¾‹å¤–ãŒéš ã‚Œã‚‹å¯èƒ½æ€§
- æœªçŸ¥ãƒˆãƒ¼ã‚¯ãƒ³ã§ KeyError ãŒç™ºç”Ÿ
- éƒ¨åˆ†çš„ãªå¤±æ•—ã‚’æ¤œçŸ¥ã§ããªã„

**ä¿®æ­£æ–¹æ³•**:
â†’ **ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨è©³ç´°ãƒ­ã‚®ãƒ³ã‚°**
```python
try:
    for token, weight in sparse_vector.items():
        if token not in token2id:
            logger.warning(f"Unknown token: {token}")
            continue
        indices.append(token2id[token])
except KeyError as e:
    logger.error(f"Token lookup failed: {e}")
```

---

## ç¬¬3æ®µéš: é€£é–çš„ä¿®æ­£ï¼ˆChain of Revisionï¼‰

### ğŸš€ å®Ÿè£…ã—ãŸæ”¹å–„

#### **A. ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã®å¼·åŒ–**

```python
# Thread-local encoder storage
_encoder_local = threading.local()

def get_or_create_thread_local_encoder(encoder):
    if not hasattr(_encoder_local, 'encoder'):
        _encoder_local.encoder = encoder
    return _encoder_local.encoder
```

#### **B. é †åºä¿è¨¼å‹ã®ä¸¦åˆ—å‡¦ç†**

```python
# ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã« Future ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
future_to_chunk_idx = {
    executor.submit(_encode_batch_documents, chunk_docs, token2id, offset): idx
    for idx, (chunk_docs, offset) in enumerate(chunks)
}

# å®Œäº†é †åºã«é–¢ã‚ã‚‰ãšã€å…ƒã®é †åºã‚’å¾©å…ƒ
points_dict = {}
for future in as_completed(future_to_chunk_idx):
    chunk_idx = future_to_chunk_idx[future]
    points_dict[chunk_idx] = future.result()

points = [points_dict[i] for i in sorted(points_dict.keys())]
```

#### **C. å‹•çš„ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®è¨ˆç®—**

```python
num_workers = config.max_workers or min(4, cpu_count())
chunk_size = max(1, len(docs) // (num_workers * 2))  # ç´°ç²’åº¦ãƒãƒ£ãƒ³ã‚¯
```

#### **D. è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å°å…¥**

```python
@dataclass
class EncodingConfig:
    batch_size: int = 1000
    max_workers: Optional[int] = None
    use_parallel: bool = True
    parallel_threshold: int = 100
    timeout_seconds: float = 300.0
    retry_attempts: int = 3
```

#### **E. åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**

```python
try:
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        ...
except Exception as e:
    logger.error(f"Parallel processing failed: {e}")
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†
    return _encode_batch_documents(encoder, docs, token2id)
```

---

## æ”¹å–„ã®åŠ¹æœ

### å‰å¾Œæ¯”è¼ƒ

| é …ç›® | æ”¹å–„å‰ | æ”¹å–„å¾Œ | å‚™è€ƒ |
|------|-------|-------|------|
| **ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£** | âš ï¸ æœªæ¤œè¨¼ | âœ… ä¿è¨¼ | Thread-local storage |
| **ID é †åº** | âŒ ä¸ç¢ºå®š | âœ… ä¿è¨¼ | Chunk mapping |
| **ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡** | ğŸŸ¡ æ¡ä»¶ä¾å­˜ | âœ… æœ€é© | Model path ã‚­ãƒ¼ |
| **ID è¡çª** | ğŸ”´ ãƒªã‚¹ã‚¯ | âœ… è§£æ±º | ID offset |
| **ã‚¨ãƒ©ãƒ¼å‡¦ç†** | âŒ ãªã— | âœ… å®Œå…¨ | Try-catch + fallback |
| **å¯è¦³æ¸¬æ€§** | ğŸŸ¡ æœ€å°é™ | âœ… å……å®Ÿ | è©³ç´°ãƒ­ã‚®ãƒ³ã‚° |

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

**å®Ÿæ¸¬ãŒå¿…é ˆ** - ç†è«–å€¤ã‹ã‚‰å®Ÿè£…ã¸:

```
ã€ç†è«–å€¤ (åˆå›æ¡ˆ)ã€‘
- å®Ÿè¡Œæ™‚é–“: 8.5s â†’ 6.2s (-27%)
- ãƒ¡ãƒ¢ãƒª: 850MB â†’ 780MB (-8%)

ã€å®Ÿè£…å€¤ (æ”¹å–„å¾Œ)ã€‘
- ã‚¹ãƒ¬ãƒƒãƒ‰åŒ–ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è€ƒæ…®
- ãƒãƒ£ãƒ³ã‚¯åŒ–ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
- å®‰å®šæ€§ (reliability) ã®ç²å¾—

â€» å®Ÿéš›ã®æ”¹å–„ç‡ã¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æ¤œè¨¼
```

---

## æ–°æ©Ÿèƒ½

### 1. EncodingConfig
```python
config = EncodingConfig(
    batch_size=1000,
    max_workers=4,
    use_parallel=True,
)
```

### 2. ãƒ­ã‚®ãƒ³ã‚°
```python
logger.info(f"Encoded {len(points)} documents")
logger.warning(f"Unknown token: {token}")
logger.error(f"Encoding failed: {e}")
```

### 3. ID ã‚ªãƒ•ã‚»ãƒƒãƒˆç®¡ç†
```python
for offset, batch in encode_documents2points_batched(encoder, docs, id_offset=1000):
    # batch ã® ID ã¯ 1000 ã‹ã‚‰å§‹ã¾ã‚‹
```

### 4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
```python
try:
    points = encode_documents2points(encoder, docs)
except Exception as e:
    # é †åºãŒä¿è¨¼ã•ã‚ŒãŸå®‰å…¨ãªçŠ¶æ…‹
    pass
```

---

## ãƒ†ã‚¹ãƒˆ

å®Ÿè£…ã«ã¯åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å«ã‚€ (`test_encode.py`):

- âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ
- âœ… æœªçŸ¥ãƒˆãƒ¼ã‚¯ãƒ³ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- âœ… é †åºä¿è¨¼ãƒ†ã‚¹ãƒˆ
- âœ… ID è¡çªé˜²æ­¢ãƒ†ã‚¹ãƒˆ
- âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- âœ… å…¥åŠ›æ¤œè¨¼

---

## ã¾ã¨ã‚: 60ç‚¹ â†’ 100ç‚¹ã¸ã®é€²åŒ–

### åˆå›æ¡ˆï¼ˆ60ç‚¹ï¼‰ã®èª²é¡Œ
1. ä¸¦åˆ—åŒ–ã®å®ŸåŠ¹æ€§æœªæ¤œè¨¼
2. ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãªã—
3. ID é †åºã®ä¸ç¢ºå®šæ€§
4. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®éåŠ¹ç‡
5. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãªã—

### æ”¹å–„ç‰ˆï¼ˆ100ç‚¹ï¼‰ã§ã®è§£æ±º
1. âœ… ãƒãƒ£ãƒ³ã‚¯åŒ–ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
2. âœ… Thread-local storage ã§ä¿è¨¼
3. âœ… Future mapping ã§é †åºä¿è¨¼
4. âœ… Model path ã‚­ãƒ¼ã§æœ€é©åŒ–
5. âœ… Try-catch + Fallback

---

**Chain of Hindsight** ã‚’é€šã˜ã¦ã€åˆå›æ¡ˆã®æ€ã„ãŒã‘ãªã„è½ã¨ã—ç©´ã‚’ç™ºè¦‹ã—ã€æœ¬ç•ªç’°å¢ƒã§ã®å …ç‰¢æ€§ã‚’å¤§ããå‘ä¸Šã•ã›ã¾ã—ãŸã€‚

