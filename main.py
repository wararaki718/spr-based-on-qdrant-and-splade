"""Main pipeline for SPLADE + Qdrant with production-ready error handling.

This module demonstrates batch upsert with exponential backoff retry logic,
drawing from PR #4 best practices for reliability and PR #3 logging patterns.
"""

import logging
import sys
from typing import List

from light_splade import SpladeEncoder
from qdrant_client import QdrantClient

from batch_upsert import (
    BatchUpsertConfig,
    create_collection_safe,
    delete_collection_safe,
    upsert_points_batched,
)
from encode import encode_documents2points, encode_query2vector
from utils import show_results

# Configure logging for observability (from PR #3)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


def main(
    qdrant_url: str = "http://localhost:6333",
    collection_name: str = "sparse_splade_collection",
    batch_size: int = 100,
    max_retries: int = 3,
) -> None:
    """Main pipeline for SPLADE + Qdrant batch processing.

    Args:
        qdrant_url: URL of the Qdrant server.
        collection_name: Name of the collection to create.
        batch_size: Points per batch for upsert.
        max_retries: Maximum retry attempts for transient failures.
    """
    logger.info("=" * 80)
    logger.info("Starting SPLADE + Qdrant Pipeline (Production-Ready)")
    logger.info("=" * 80)

    # Initialize clients
    try:
        logger.info(f"Connecting to Qdrant at {qdrant_url}...")
        client = QdrantClient(url=qdrant_url)
        logger.info("âœ… Qdrant connection successful")
    except Exception as e:
        logger.error(f"âŒ Failed to connect to Qdrant: {e}", exc_info=True)
        sys.exit(1)

    try:
        logger.info("Loading SPLADE encoder...")
        encoder = SpladeEncoder(model_path="bizreach-inc/light-splade-japanese-28M")
        logger.info("âœ… SPLADE encoder loaded")
    except Exception as e:
        logger.error(f"âŒ Failed to load SPLADE encoder: {e}", exc_info=True)
        sys.exit(1)

    # Sample documents
    docs: List[str] = [
        "Qdrantã¯é«˜é€Ÿãªãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã§ã™",
        "SPLADEã¯ã‚¹ãƒ‘ãƒ¼ã‚¹è¡¨ç¾ã‚’å­¦ç¿’ã—ã¾ã™",
        "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ä»•çµ„ã¿ã‚’ç†è§£ã—ã¾ã—ã‚‡ã†",
        "Pythonã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™",
        "Qdrantã¨SPLADEã‚’çµ„ã¿åˆã‚ã›ã¦ä½¿ã„ã¾ã™",
    ]

    logger.info(f"Processing {len(docs)} documents for indexing...")

    # Step 1: Create collection (with graceful already-exists handling)
    try:
        created = create_collection_safe(
            client, collection_name, sparse_vector_config_name="text-sparse"
        )
        if created:
            logger.info(f"âœ… Created new collection '{collection_name}'")
        else:
            logger.info(f"âš ï¸  Collection '{collection_name}' already exists (will reuse)")
    except RuntimeError as e:
        logger.error(f"âŒ {e}", exc_info=True)
        sys.exit(1)

    # Step 2: Encode documents
    try:
        logger.info("Encoding documents to sparse vectors...")
        points = encode_documents2points(encoder, docs)
        logger.info(f"âœ… Encoded {len(points)} points")
    except Exception as e:
        logger.error(f"âŒ Failed to encode documents: {e}", exc_info=True)
        sys.exit(1)

    # Step 3: Batch upsert with exponential backoff (PR #4 integration)
    try:
        logger.info(f"Starting batch upsert (batch_size={batch_size})...")
        config = BatchUpsertConfig(
            batch_size=batch_size,
            max_retries=max_retries,
            retry_delay_seconds=1.0,
        )
        upserted_count = upsert_points_batched(
            client, collection_name, points, config
        )

        if upserted_count == len(points):
            logger.info(f"âœ… All {upserted_count} points upserted successfully")
        else:
            logger.warning(
                f"âš ï¸  Partial success: {upserted_count}/{len(points)} points upserted"
            )
    except Exception as e:
        logger.error(f"âŒ Batch upsert failed: {e}", exc_info=True)
        sys.exit(1)

    # Step 4: Search (verify indexing worked)
    try:
        query = "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ä»•çµ„ã¿"
        logger.info(f"Performing search query: '{query}'")

        query_sparse_vector = encode_query2vector(encoder, query)
        logger.info("âœ… Query encoded")

        search_result = client.query_points(
            collection_name=collection_name,
            query=query_sparse_vector,
            using="text-sparse",
            limit=3,
        )
        logger.info(f"âœ… Found {len(search_result.points)} search results")

        if search_result.points:
            logger.info("\nğŸ“Š Top Search Results:")
            show_results(search_result.points)
        else:
            logger.warning("No search results found")

    except Exception as e:
        logger.error(f"âŒ Search failed: {e}", exc_info=True)
        sys.exit(1)

    # Step 5: Cleanup (with graceful not-found handling)
    try:
        logger.info(f"Cleaning up collection '{collection_name}'...")
        deleted = delete_collection_safe(client, collection_name)
        if deleted:
            logger.info(f"âœ… Deleted collection '{collection_name}'")
        else:
            logger.warning(f"âš ï¸  Collection '{collection_name}' not found (already deleted)")
    except RuntimeError as e:
        logger.error(f"âŒ {e}", exc_info=True)
        # Don't exit here - cleanup failure is not critical

    logger.info("=" * 80)
    logger.info("âœ… Pipeline completed successfully!")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()
